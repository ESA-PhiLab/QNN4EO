{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QCNN-RealAmplitudesCircuit-Group1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cpMbTWRx0yBl"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uc2xlyNfycy6"},"source":["!pip uninstall qiskit\n","!pip install qiskit==0.23.0\n","#!pip install git+https://github.com/qiskit-community/qiskit-textbook.git#subdirectory=qiskit-textbook-src\n","!pip install pylatexenc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXsPVL2Qyt4G"},"source":["from qiskit import execute\n","from qiskit.circuit import Parameter,ControlledGate\n","from qiskit import Aer\n","import qiskit\n","import numpy as np\n","from tqdm import tqdm\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import torch\n","from torch.autograd import Function\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda:0\")\n","  print(\"Running on the GPU\")\n","else:\n","  device = torch.device(\"cpu\")\n","  print(\"Running on the CPU\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLM7EOjvzYNa"},"source":["#np.random.seed = 314\n","\n","NUM_QUBITS = 4\n","NUM_SHOTS = 800 #3000\n","SHIFT = np.pi/4\n","LEARNING_RATE = 0.0002\n","MOMENTUM = 0.5\n","\n","SIMULATOR = Aer.get_backend('qasm_simulator')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjFSm0lgzbW2"},"source":["# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n","import itertools\n","def create_QC_OUTPUTS():\n","    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n","    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n","\n","QC_OUTPUTS = create_QC_OUTPUTS()\n","print(QC_OUTPUTS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5MjzswxFOf5A"},"source":["# Circuit"]},{"cell_type":"code","metadata":{"id":"MKJrEyNCzdX_"},"source":["class QiskitCircuit():\n","    \n","    def __init__(self, n_qubits, backend, shots):\n","        # --- Circuit definition ---\n","        self.circuit = qiskit.QuantumCircuit(n_qubits)\n","        self.n_qubits = n_qubits\n","        self.thetas = {k : Parameter('Theta'+str(k))for k in range(2*self.n_qubits)}\n","        \n","        all_qubits = [i for i in range(n_qubits)]\n","        self.circuit.h(all_qubits)\n","        self.circuit.barrier()\n","\n","        #self.circuit.h(0)\n","\n","        for k in range(0, 4):\n","          self.circuit.ry(self.thetas[k], k)\n","\n","        self.circuit.cx(0, 1)\n","        self.circuit.cx(0, 2)\n","        self.circuit.cx(0, 3)\n","        self.circuit.cx(1, 2)\n","        self.circuit.cx(1, 2)\n","        self.circuit.cx(2, 3)\n","\n","        for k in range(0, 4):\n","          self.circuit.ry(self.thetas[k+4], k)\n","\n","        self.circuit.measure_all()\n","        #self.circuit.measure(1,0)\n","        # ---------------------------\n","        \n","        self.backend = backend\n","        self.shots = shots\n","        \n","        \n","    def N_qubit_expectation_Z(self,counts, shots, nr_qubits):\n","        expects = np.zeros(len(QC_OUTPUTS))\n","        for k in range(len(QC_OUTPUTS)):\n","            key = QC_OUTPUTS[k]\n","            perc = counts.get(key, 0)/shots\n","            expects[k] = perc\n","        return expects\n","    \n","    def run(self, i):\n","        params = i\n","        #print('params = {}'.format(len(params)))\n","        backend = Aer.get_backend('qasm_simulator')\n","    \n","        job_sim = execute(self.circuit,\n","                              self.backend,\n","                              shots=self.shots,\n","                              parameter_binds = [{self.thetas[k] : params[k].item() for k in range(2*NUM_QUBITS)}])\n","#         \n","        result_sim = job_sim.result()\n","        counts = result_sim.get_counts(self.circuit)\n","        return self.N_qubit_expectation_Z(counts, self.shots, NUM_QUBITS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Afi3vEahKBVU"},"source":["circuit = QiskitCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n","#print('Expected value for rotation [pi/4]: {}'.format(circuit.run(torch.Tensor([np.pi/4]*NUM_QUBITS))))\n","circuit.circuit.draw(output='mpl')#, filename='Figures/{}-qubit circuit ryN.jpg'.format(NUM_QUBITS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zDbKVw5zhWo"},"source":["class TorchCircuit(Function):    \n","\n","    @staticmethod\n","    def forward(ctx, i):\n","        if not hasattr(ctx, 'QiskitCirc'):\n","            ctx.QiskitCirc = QiskitCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n","            \n","        exp_value = ctx.QiskitCirc.run(i)\n","        \n","        result = torch.tensor([exp_value])\n","  \n","        ctx.save_for_backward(result, i)\n","        \n","        return result\n","    \n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \n","        forward_tensor, i = ctx.saved_tensors\n","#         print('forward_tensor = {}'.format(forward_tensor))\n","        input_numbers = i\n","#         print('input_numbers = {}'.format(input_numbers))\n","        gradients = torch.Tensor()\n","        \n","        for k in range(2*NUM_QUBITS):\n","            shift_right = input_numbers.detach().clone()\n","            shift_right[k] = shift_right[k] + SHIFT\n","            shift_left = input_numbers.detach().clone()\n","            shift_left[k] = shift_left[k] - SHIFT\n","            \n","#             print('shift_right = {}, shift_left = {}'.format(shift_right, shift_left))\n","            \n","            expectation_right = ctx.QiskitCirc.run(shift_right)\n","            expectation_left  = ctx.QiskitCirc.run(shift_left)\n","#             print('expectation_right = {}, \\nexpectation_left = {}'.format(expectation_right, expectation_left))\n","            \n","            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n","            # rescale gradient\n","#             gradient = gradient / torch.norm(gradient)\n","#             print('gradient for k={}: {}'.format(k, gradient))\n","            gradients = torch.cat((gradients, gradient.float()))\n","            \n","        result = torch.Tensor(gradients)\n","#         print('gradients = {}'.format(result))\n","#         print('grad_output = {}'.format(grad_output))\n","\n","        return (result.float() * grad_output.float()).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjTZRiru0eKr"},"source":["import sys\n","sys.path.append('/content/drive/MyDrive/QAI4EO/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"An1kDAd-0f1q"},"source":["from DatasetHandler import DatasetHandler\n","dataset_root = '/content/drive/MyDrive/QAI4EO/datasets/EuroSAT/EuroSAT'\n","handler = DatasetHandler(dataset_root)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcZfoXab0hDP"},"source":["classes = []\n","for i, c in enumerate(handler.classes):\n","  cl = c.split('/')[-1]\n","  classes.append(cl)\n","classes.sort()\n","classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_0eBIXUsNxz"},"source":["classes = ['AnnualCrop', 'PermanentCrop', 'Pasture', 'Forest', 'HerbaceousVegetation']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tTMKedxr0tCP"},"source":["imgs_path, imgs_label = handler.load_paths_labels(dataset_root, classes=classes)\n","print('Dataset images:', len(imgs_path), 'Dataset labels:', len(imgs_label))\n","print('Dataset sample ->', imgs_path[0], imgs_label[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ku3ztjPT0urb"},"source":["train_imgs, train_labels, val_images, val_labels = handler.train_validation_split(imgs_path, imgs_label, split_factor=0.2)\n","print('X_train shape:', train_imgs.shape, 'Y_train shape:', train_labels.shape)\n","print('  X_val shape: ', val_images.shape, '  Y_val shape: ', val_labels.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vT_pbek9tZqx"},"source":["for i in range(len(train_labels)):\n","  lr = train_labels[i]\n","  l = 0\n","\n","  if lr == 0:\n","    l = 0\n","  elif lr == 1:\n","    l = 1\n","  elif lr == 2:\n","    l = 2\n","  elif lr == 5:\n","    l = 3\n","  elif lr == 6: \n","    l = 4\n","\n","  train_labels[i] = l\n","\n","for i in range(len(val_labels)):\n","  lr = val_labels[i]\n","  l = 0\n","\n","  if lr == 0:\n","    l = 0\n","  elif lr == 1:\n","    l = 1\n","  elif lr == 2:\n","    l = 2\n","  elif lr == 5:\n","    l = 3\n","  elif lr == 6: \n","    l = 4\n","  \n","  val_labels[i] = l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"InesYmyx0wgc"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n","\n","        #self.conv2_drop = nn.Dropout2d()\n","\n","        self.fc1 = nn.Linear(2304, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 42)\n","\n","        self.fc4 = nn.Linear(2304, 2*NUM_QUBITS)\n","        \n","        self.qc = TorchCircuit.apply\n","\n","        #self.fc5 = nn.Linear(16, 10)\n","        self.fc5 = nn.Linear(16, 5)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n","\n","        x = x.view(-1, 2304)\n","        #x = F.relu(self.fc1(x))\n","        #x = F.dropout(x, training=self.training)\n","        #x = F.relu(self.fc2(x))\n","\n","        #x = F.relu(self.fc3(x))\n","\n","        x = self.fc4(x)\n","        x = np.pi*torch.tanh(x)\n","        \n","        x = self.qc(x[0]) # QUANTUM LAYER\n","        \n","        x = F.relu(x)\n","        #print('output of QC = {}'.format(x))\n","        \n","#         # softmax rather than sigmoid\n","        x = self.fc5(x.float())\n","        #print('output of Linear(1, 2): {}'.format(x))\n","        x = F.softmax(x, 1)\n","\n","        #x = torch.sigmoid(x)\n","        #x = torch.cat((x, 1-x), -1)\n","        return x\n","    \n","    \n","    def predict(self, x):\n","        # apply softmax\n","        pred = self.forward(x)\n","#         print(pred)\n","        ans = torch.argmax(pred[0]).item()\n","        return torch.tensor(ans)\n","    \n","network = Net()#.to(device)\n","optimizer = optim.Adam(network.parameters(), lr=0.0002)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oMzCmz11PVo"},"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n","\n","#from torchsummary import summary\n","#summary(network, (3, 64, 64))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxkBEGs401rE"},"source":["train_loader = iter(handler.qcnn_data_loader(train_imgs, train_labels, batch_size = 1, img_shape = (64,64,3)))\n","test_loader = iter(handler.qcnn_data_loader(val_images, val_labels, batch_size = 1, img_shape = (64,64,3)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhLeUsW1_WpK"},"source":["checkpoint = torch.load('/content/drive/MyDrive/QAI4EO/model-real-amp-group1.pt')\n","network.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUoZidbl05zA"},"source":["train_loss_list = []\n","val_loss_list = []\n","epochs = 25\n","\n","loss_func = nn.CrossEntropyLoss()\n","\n","for epoch in range(epochs):\n","  train_loader = iter(handler.qcnn_data_loader(train_imgs, train_labels, batch_size = 1, img_shape = (64,64,3)))\n","  test_loader = iter(handler.qcnn_data_loader(val_images, val_labels, batch_size = 1, img_shape = (64,64,3)))\n","  total_loss = []\n","  for batch_idx in range(len(train_labels)):\n","    data, target = next(train_loader) \n","    # print(batch_idx)\n","    optimizer.zero_grad()        \n","    # Forward pass\n","    output = network(data)\n","    # Calculating loss\n","    loss = loss_func(output, target)\n","    # Backward pass\n","    loss.backward()\n","    # Optimize the weights\n","    optimizer.step()\n","    \n","    total_loss.append(loss.item())\n","    \n","    print('\\r Epoch %d ~ Batch %d (%d) ~ Loss %f ' % (epoch, batch_idx, len(train_imgs)-1, loss.item()), end='\\t\\t')\n","    \n","  \n","  with torch.no_grad():\n","    val_loss = []\n","    targets = []\n","    predictions = []\n","    for batch_idx in range(len(val_images)):\n","      data, target = next(test_loader)\n","      output = network(data)\n","      loss = loss_func(output, target)\n","      val_loss.append(loss.item())\n","\n","      targets.append(target.item())\n","      \n","      predictions.append(network.predict(data).item())\n","  \n","      \n","  train_loss_list.append(sum(total_loss)/len(total_loss))\n","  val_loss_list.append(sum(val_loss)/len(val_loss))\n","  \n","  print('Training [{:.0f}%]\\t Training Loss: {:.4f} Validation Loss: {:.4f}'.format(\n","      100. * (epoch + 1) / epochs, train_loss_list[-1], val_loss_list[-1]))\n","  \n","  if epoch % 3 == 1:\n","    print(confusion_matrix(targets, predictions,normalize='true'))\n","    print(classification_report(targets, predictions, target_names=classes, digits=4))\n","    torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': network.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': train_loss_list[-1],\n","            }, '/content/drive/MyDrive/QAI4EO/model-real-amp-group1-2.pt')\n","    #torch.save(network.state_dict(), '/content/drive/MyDrive/QAI4EO/model-bell.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tyaee9pf1fxF"},"source":["fig, plt.plot(train_loss_list)\n","plt.plot(val_loss_list)\n","plt.title('Hybrid NN Training Convergence for {}-qubit'.format(NUM_QUBITS))\n","plt.xlabel('Training Iterations')\n","plt.ylabel('Cross Entropy Loss')\n","plt.legend(['Training', 'Validation'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EmCnfM2ZoB9t"},"source":["# Save tests "]},{"cell_type":"code","metadata":{"id":"RCuHYMyloi31"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mBJoclTMRqf2"},"source":["checkpoint = torch.load('/content/drive/MyDrive/QAI4EO/model-real-amp-group1-2.pt')\n","network.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7kFz4UDoguS"},"source":["coarse_db = pd.read_csv('/content/drive/MyDrive/QAI4EO/coarse_classification.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XGUoLBufovEG"},"source":["coarse_db.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jetRK7iZotME"},"source":["val_images = coarse_db.PATH.values\n","full_targets = coarse_db.FULLTARGET.values\n","coarse_prediction = coarse_db.PREDICTION.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L39Gc3_tocfy"},"source":["batch_in = np.zeros((1, 3, 64, 64))\n","\n","full_targets2 = []\n","\n","with torch.no_grad():\n","  predictions = []\n","  for batch_idx in range(len(val_images)):\n","\n","    if coarse_prediction[batch_idx] == 0:\n","      batch_in[0,...] = np.transpose(plt.imread(val_images[batch_idx])/255.0)\n","      output = network.predict(torch.Tensor(batch_in)).item()\n","      predictions.append(output)\n","      full_targets2.append(full_targets[batch_idx])\n","\n","    print('\\r  IMG: %d of %d' % (batch_idx, len(val_images)), end='\\t\\t')\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apWPG5CSuWuZ"},"source":["len(full_targets2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Z_NbrmIvLxk"},"source":["full_targets3 = []\n","\n","for i in range(len(full_targets2)):\n","  lr = full_targets2[i]\n","  \n","  l = 0\n","\n","  if lr == 0:\n","    l = 0\n","  elif lr == 1:\n","    l = 1\n","  elif lr == 2:\n","    l = 2\n","  elif lr == 5:\n","    l = 3\n","  elif lr == 6: \n","    l = 4\n","\n","  full_targets3.append(l)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rZvtE_fuYJF"},"source":["print(confusion_matrix(full_targets3, predictions,normalize='true'))\n","print(classification_report(full_targets3, predictions, target_names=['Annual Crop','Forest', 'HerbaceousVegetation', 'Pasture', 'PermanentCrop'], digits=4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdll59fyu5k0"},"source":[""],"execution_count":null,"outputs":[]}]}